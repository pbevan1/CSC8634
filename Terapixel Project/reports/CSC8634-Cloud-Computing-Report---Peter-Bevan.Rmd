---
title: "CSC8634 Cloud Computing Report"
author: "Peter Bevan"
date: "18/01/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
```

```{r load project, include=FALSE}
library(ProjectTemplate)
load.project()
```


```{r, echo=FALSE, fig.cap="Terapixel Rendering of Newcastle upon Tyne", out.width = '100%', fig.align='center'}
knitr::include_graphics(here("Terapixel Project/graphs/ncl2.png"))
```


## Business Understanding

This report documents an exploratory data analysis of application checkpoint and system metric output data collected during the rendering of a 3d terapixel image of the city of Newcastle, UK. A terapixel is defined as 10^12 pixels (1,000,000 megapixels), and so are extremely large images and therefore very computationally costly to produce. A path tracing renderer coupled with a scalable public cloud supercomputing architecture was successfully used to render the terapixel image in under a day using 1024 public cloud GPU nodes. The completed terapixel image is accessible for users to explore the city in 3D with zoomable scales ranging from a complete overview to minute detail.

Since the rendering of such large images is extremely demanding, optimization of the architecture and process is essential for keeping the time frames manageable. Since the rendering is carried out on the public cloud on a pay as you go basis, this optimization also helps to keep costs down, especially if continuous updating of the visualization is to take place. One way to increase the rendering speed is to **select the specific GPU cards which perform best for the required task.** This is what this report aims to achieve.

The data collected during the initial rendering can be analysed to view the performance of each type of GPU card in order to decide on the optimal cards for the task, as well as revealing which GPUs seem to be unsuitable for the task. The data can be manipulated to allow easy comparison of GPU performance, followed by dimensionality reduction techniques and clustering to separate the desirable and undesirable GPU cards.

The tools used in the project were Rstudio and Rmarkdown combined with various packages (see config file for details on packages). ProjectTemplate was used to organise and link the files, and Git/Github was used for version control.

## Data Understanding

The data was downloaded directly from the Newcastle data science student projects GitHub repository, which is a private repository. The data comprises 3 large datasets: `application.checkpoints`, `gpu` and `task.x.y`. The shape of this data is shown below:

```{r dim}
dim(application.checkpoints)
dim(gpu)
dim(task.x.y)
```
It's clear that the data is very large, with over 650 thousand observation in the `application.checkpoints` dataset across 6 variables, over 1.5 million observations in the `gpu` dataset across 8 variables, and over 65 thousand observations across 5 variables in the `task.x.y` dataset. Each dataset is summarised below:

### application.checkpoints 
The `application.checkpoints` data is a comprehensive record of timestamped checkpoint events from the rendering process, with one timestamped entry for the start of the event and one for the end. The checkpoint events are as follows (taken from the `eventName` column):
* `TotalRender`: entire rendering task (excluding tiling)
* `Render`: rendering of image tile
* `Saving Config`: configuration overhead
* `Uploading`: uploading of output to Azure Blob Storage
* `Tiling`: post processing of rendered tile
  
```{r head_appplication.checkpoints}
#printing head of application checkpoint data
head(application.checkpoints)
```

### gpu 
The `gpu` data comprises periodical recording of status of the GPU on each virtual machine. Recorded GPU metrics are as below, which are paired with the timestamp of the measurement, the hostname of the virtual machine, the GPU card serial number and the system ID assigned to the GPU card:
  + `powerDrawWatt`: power draw of the GPU card in Watts
  + `gpuTempC`: temperature of GPU in Celsius at specified timestamp
  + `gpuUtilPerc`: utilisation of the GPU **core/s** (%)
  + `gpuMemUtilPerc`: utilisation of the GPU **memory** (%)

```{r head_gpu}
#printing head of gpu data
head(gpu)
```

### task.x.y
the x and y coordinates of the section of image being rendered for each task. `level` represents the zoom level, of the 12 zoom levels of the visualisation (12 being most zoomed in and 1 being completely zoomed out). In the dataset there is only levels 4, 8 and 12 since the other levels are derived in the tiling process.

```{r head_task.x.y}
#printing head of task.x.y data
head(task.x.y)
```

Each dataset was visualised with the `naniar` library, using `vis_miss` to quickly get an overview of any missing values. None were present in any of the datasets and so the visualisations were omitted from this report. This lack of missing data is likely because the data is mostly machine collected measurements and so the data was consistently recorded.


## Data Preparation

Each event type was separated from `application.checkpoints` and grouped by task ID (one group per stop and start of each task). These paired start and stop times were then converted to the duration of each task by finding the absolute difference between the two timestamps in each pair using `difftime`. This resulted in 5 new dataframes, one for each event type previously listed under `eventName`. These new dataframes were then joined by `taskId` and `hostname` to create a dataframe containing the runtime breakdowns for each event of each rendering task. It's clear from summing the individual event runtimes for a sample of tasks that `TotalRender` comprises all events except for `Tiling`. It was decided that a more useful metric would be a total render time that also included `Tiling` and so a new variable called `TotalRenderIncTil` was created using the sum of `Tiling`, `Uploading`, `Saving_Config` and `Render`.

The `task.x.y` data was merged with the runtime data described above by `taskId` to allow the tile position and level of each task to be referenced if necessary.

There are 1024 different hostnames, which means 1024 different GPUs. It can be seen after filtering by level and grouping by hostname that all 1024 virtual machines in the dataset has carried out rendering on level 12, with 256 of these also rendering on level 8 and only one rendering on level 4. The sum total of the duration of the rendering for each task and level, was calculated and is shown below. This shows that the vast majority of the computing is done rendering level 12. The avenue of most value is likely in focusing analysis on this with the hope of generating insights that lead to optimisation which could reduce the time/money/energy spent in the future. The remainder of the analysis in this report focuses solely on level 12 rendering, choosing to omit the level 4 and 8 with the aim of keeping the data comparable for analysis.

```{r render totals, echo=FALSE}
str_glue('Summed total render runtime for level 12 accross all parallel GPUs: {as.numeric(sum(Runtimes_12$TotalRender_IncTil))/60/60} hours')
str_glue('Summed total render runtime for level 8 accross all parallel GPUs: {as.numeric(sum(Runtimes_8$TotalRender_IncTil))/60/60} hours')
str_glue('Summed total render runtime for level 4 accross all parallel GPUs: {as.numeric(sum(Runtimes_4$TotalRender_IncTil))/60/60} hours')
```

Summary data showing mean values for each gpu based on the `gpu` and `Runtimes` data were calculated and stored separately for each level. The arithmetic mean was used to calculate cost values whereas the Geometric mean was used to summarise the percentage utilisation of the GPU memory and core(s) since these are ratios and we have no access to the costs that these ratios are based on. (Hoefler 2015) Since both of these summaries boiled the data down to one entry per gpu, they were then easily merged to create the `gpu_performance`. The head of this final dataset is shown below, noting that there is 1024 observations (1 per GPU node):

```{r gpu_performance_12}
head(gpu_performance_12)
```

To conclude the data munging, runtime totals were calculated to show the sum total runtime of each gpu for each event type, which allows the dominance of each event type to be visualised.

# Modeling

```{r Histograms, echo=FALSE, message=FALSE}
plt_power = ggplot(gpu_performance_12, aes(x=mean_powerDrawWatt)) + 
  geom_histogram(color='white') + xlab('Mean power draw of GPU (Watts)') +
  theme(plot.title = element_text(hjust = 0.5))

plt_temp = ggplot(gpu_performance_12, aes(x=mean_gpuTempC)) + 
    geom_histogram(color='white', binwidth=0.5) + xlab('Mean temp of GPU (C)') +
  theme(plot.title = element_text(hjust = 0.5))

#Clearly multimodal (different classes of GPU??)
plt_utilPerc = ggplot(gpu_performance_12, aes(x=mean_gpuUtilPerc)) + 
    geom_histogram(color='white') + xlab('Mean utilisation of GPU Core(s) (%)') +
  theme(plot.title = element_text(hjust = 0.5))

plt_memUtilPerc = ggplot(gpu_performance_12, aes(x=mean_gpuMemUtilPerc)) + 
    geom_histogram(color='white') + xlab('Mean utilisation of GPU memory (%)') +
  theme(plot.title = element_text(hjust = 0.5))

#multimodal as well - pull out lower group
plt_TotalRender_IncTile = ggplot(gpu_performance_12, aes(x=mean_TotalRender_IncTil)) + 
    geom_histogram(color="#D55E00") + xlab('Mean duration of total render task (seconds)') +
  theme(plot.title = element_text(hjust = 0.5))

plt_Tiling = ggplot(gpu_performance_12, aes(x=mean_Tiling)) + 
    geom_histogram(color="#F0E442", binwidth=0.01) + xlab('Mean duration of tiling (seconds)') +
  theme(plot.title = element_text(hjust = 0.5))

plt_SavingConfig = ggplot(gpu_performance_12, aes(x=mean_SavingConfig)) + 
    geom_histogram(color="#56B4E9", binwidth=0.0001) + xlab('Mean duration of configuration overhead (seconds)') +
  theme(plot.title = element_text(hjust = 0.5)) + theme(text = element_text(size=10))

plt_Render = ggplot(gpu_performance_12, aes(x=mean_Render)) + 
    geom_histogram(color="#009E73", binwidth=0.5) + xlab('Mean duration of rendering (seconds)') +
  theme(plot.title = element_text(hjust = 0.5))

plt_Uploading = ggplot(gpu_performance_12, aes(x=mean_Uploading)) + 
    geom_histogram(color="#0072B2", binwidth=0.03) + xlab('Mean duration of uploading (seconds)') +
  theme(plot.title = element_text(hjust = 0.5))
```

Histograms were plotted to explore the distributions of each variable and displayed on separate grids for rendering durations and gpu data. There is an interesting bi-modal distribution for the `mean_TotalRenderIncTil` variable, as well as for `mean_Render`. This may be suggesting that there are an equal number of two different types of GPU which are rendering at different speeds, which is then affecting the total render time. There looks to be equal numbers of gpu's in each group judging by the size and shape of the distributions about each mode, potentially pointing towards the gpu's used being half one type and half another. Further exploration is needed to confirm this suggestion.
Mean tiling time and mean configuration overhead look to be following roughly normal distributions, with a small number of outliers. Mean uploading duration is multimodal with three distinct peaks suggesting that the gpus can be separated into three groups based on uploading speed.

```{r rendergrid, echo=FALSE, message=FALSE, fig.cap='Rendering event histograms'}
grid.arrange(plt_TotalRender_IncTile, plt_Tiling, plt_SavingConfig, plt_Render, plt_Uploading, nrow = 3)
```

The proportion of the total render time spent on each task was visualised in a pie chart to get a better understanding of what event types the GPUs are spending most time completing. This knowledge could be very useful when looking at speeding up the whole process as it may indicate an area that has most room for improvement. Rendering took up the vast majority of the total task runtimes, as shown in the pie chart, with just under 95% of the total runtime. Speeding up the rendering time by just a few percent would have a dramatic effect on the overall runtime of the process. On the contrary, the configuration overhead is so small (0.01% of total runtime) that it is not visible on the pie chart and so it's probably not worth spending too much time attempting to speed up this process (benefits likely small). A colourblind friendly palette was used, based on the codes provided by M.Okabe and K.Ito from Tokyo University in 2002. These colours have contrasting luminance and so are more easily distinguished by colourblind people than the default ggplot colours.

```{r pie, echo=FALSE, fig.cap='Proportions of task runtimes per event', out.width='65%', fig.align='center'}
ggplot(data = Runtime_Totals[c(1:4),], aes(x = "", y = Totals, fill = Event)) + 
  geom_bar(width=1, size=1, stat = "identity", color="#D55E00") +
  labs(x = NULL, y = NULL, title = 'proportions of task runtimes per event') +
  coord_polar(theta = "y") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5, size=9),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        panel.grid  = element_blank()) +
  scale_fill_manual(values=cbbPalette)
```



The histogram for mean power draw of the GPUs looks to have a roughly normal distribution with almost all observations falling between 80W and 100W, with a very small number of outliers above 100W. Mean temperature of the GPUs looks to have a right skewed normal distribution. Both mean utilisation of GPU cores and mean utilisation of GPU memory exhibit a similar bi-modal distribution as mentioned above, with similar numbers of GPUs falling into distributions around the two modes. One possible explanation is that some of the GPUs, for whatever reason, are utilising more of their memory and core(s), and this group as a result are producing faster rendering. In this case, one possible solution could be experimenting with optimisation in blender to try and get consistent/higher GPU utilisation. One possible tweak that could be made is adjusting the tile size to see if performance can be enhanced.

```{r gpugrid, echo=FALSE, message=FALSE, fig.cap='GPU metric histograms', out.width='75%', fig.align='center'}
grid.arrange(plt_power, plt_temp, plt_utilPerc, plt_memUtilPerc, nrow = 2)
```

\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\

The correlation matrix below was computed to visualise the correlation between pairs of variables in the newly constructed `gpu_performance` datasets. Since there are so many overlapping points, the transparency of the points was increased to highlight areas of overlapping points and better highlight trends. It's easy to notice even in these pairwise plots there is often distinct groups of GPUs.
Obvious positive linear relationships can be seen between `mean_gpuMemUtilPerc` and `mean_gpuUtilPerc` as well as between `mean_TotalRender` and `mean_Render`. There are also other potential linear relationships between other variables but it is harder to tell without further investigation. These linear relationships between at least some variables indicates that there is a chance the variables are not providing independent information. This means we may be able to reduce the dimensionality of the data using a technique such as principle component analysis (PCA) without losing too much of the total variation.

```{r, echo=FALSE, fig.cap="scatterplot matrix gpu performance", out.width='55%', fig.align='center'}
pairs(gpu_performance_12[,-6], col = rgb(red = 0, green = 0, blue = 0, alpha = 0.1))
```

An unsupervised learning technique can be used to group data like this into clusters. K-means clustering was selected since the clusters look to be highly spherical, which k-means is known to be able to differentiate well.

In order to search for an optimal value for k (number of clusters), the k-values 1 to 15 were plugged into the kmeans function and the within cluster sum of squares (SSw) recorded. The k values were then plotted against the SSw values, producing the plot below. A slight kink can be seen at k=2 and k=4, and so it was inferred that these could be good k-values.

```{r kmeans grid, echo=FALSE, out.width='55%', fig.align='center'}
#set seed to make analysis reproducable
set.seed(42)
## Set the maximum value of K:
Kmax = 15
## Set up a vector to store the values of SS_W for each value of K:
SS_W = numeric(Kmax)
#storing fits
km_fit = list()
## Loop over values of K from 1 to Kmax, recordingf SS_W:
for(K in 1:Kmax) {
  km_fit[[K]] = kmeans(gpu_performance_12, K, iter.max = 50, nstart = 20)
  SS_W[K] = km_fit[[K]]$tot.withinss
}
#plotting k value against the within-cluster sum of squares to get an
#indication of a good k-value to use
plot(1:Kmax, SS_W, type="l", xlab="K", ylab="SS_W")
```

Since the data is multidimensional, it is difficult to plot the data and visualise the clustering. To get around this issue, principle component analysis was carried out on the data and the first 2 principle components selected, which allows an overview the data to be plotted in two dimensions. The k-means clustering was then highlighted in the plot using different colours and symbols for each cluster. Both k=2 clustering and k=4 clustering can be seen below. The data clearly separates very well into two obvious clusters. This combined with the bimodal histograms points to the possibility that the GPU cards used by the virtual machines are all one of two models, and the clustering is grouping these two models. This could be useful if one model can be identified as performing better than the other at this particular task, as this would aid the creation of a more optimised architecture for future similar tasks.

```{r k2, echo=FALSE, fig.cap='k=2 clustering of GPU performance data in space of first two PCs', out.width='59%', fig.align='center'}
set.seed(42)
#kmeans function with K=2 as identified by above plot
km_2 = kmeans(gpu_performance_12[,-6], 2, iter.max=50, nstart=20)
#principle component analysis of gpu performance on level 12 renders
pca_gpu_perf = prcomp(gpu_performance_12[,-6], scale=TRUE)
## Plot the first PC against the second PC using the cluster allocation to set
## different colours and plotting characters for each cluster:
#k=2
(km2_pc_plot = ggplot(gpu_performance_12, aes(x=pca_gpu_perf$x[,1], y=pca_gpu_perf$x[,2])) +
  geom_point(colour=km_2$cluster, shape=km_2$cluster, size=2.5) + xlab('First PC') + ylab('Second PC') + 
  ggtitle('k=2 clustering of GPU performance data in space of first two PCs'))
```

```{r k4, echo=FALSE, fig.cap='k=4 clustering of GPU performance data in space of first two PCs', out.width='59%', fig.align='center'}
set.seed(42)
#kmeans function with K=2 as identified by above plot
km_4 = kmeans(gpu_performance_12[,-6], 4, iter.max=50, nstart=20)
#k=4 clustering
(km4_pc_plot = ggplot(gpu_performance_12, aes(x=pca_gpu_perf$x[,1], y=pca_gpu_perf$x[,2])) +
  geom_point(colour=km_4$cluster, shape=km_4$cluster, size=2.5) + xlab('First PC') + ylab('Second PC') + 
  ggtitle('k=4 clustering of GPU performance data in space of first two PCs'))
```

\

When k=2 the clusters appear to be primarily separated by the first principle component. This component can be roughly interpreted from the eigonvectors shown in the table below. The coefficients for GPU core utilisation, GPU memory utilisation, total render time and render time are all similarly large in magnitude and negative, with the other variables coming close to zero. This suggests the first principle component is a negatively correlated measure of the size of these variables. This means that on the k=2 plot the observations in the cluster on the right with larger PC1 values are likely to have smaller values of the variables mentioned. Smaller values of all of these variables are desirable and so the cluster on the right of the k=2 plot represents more desirable GPUs for the task.

The coefficients of the second principle component appear to show that it is dominated by power draw and temperature, with much larger coefficients for these variables. The large positive coefficients for these variables suggests that PC2 is a positively correlated measure of the size of the gpu power draw and gpu temperature. Since it is desirable for powerdraw and temperature values to be low for GPUs, a low PC2 score is desirable. If using the k=4 clustering, the blue cluster in the bottom right of the plot above can be identified as containing the overall best performers, with relatively low temperature and power draw (PC2) and relatively low render time and memory utilisation (PC1).

```{r pca_rot, echo=FALSE}
pca_gpu_perf$rotation[,c(1:2)]
```


The table below shows the cumulative proportion of the variance that is explained by each principal component. The table shows that the first 2 principle components account for 63% of the total variance in the data. This is ok as a low dimensional method of easily visualising the clusters, but there is a significant proportion of variance missing.

```{r pca_summary, echo=FALSE}
#summary, shows proportion of variance explained
summary(pca_gpu_perf)
```

The clustering was further visualised using a scatterplot matrix to view how the GPUs are clustered across variable pairs. The two clusters identified by the k-means clustering on the `gpu_performance_12` dataset were highlighted in different colours to show how the clusters are represented across pairs of variables. There is a much more visible separation between the clusters in k=2 for most of the pairs. The scatterplot matrix confirms the suspicions discussed above that the clustering using k=2 doesn't separate well on a lot of pairs containing power draw and temperature. The k=4 clustering seems to do better with these variables as expected from the PCA plot. There is quite a lot of overlap between clusters involving uploading speed for both k=2 and k=4, suggesting that the upload speed is not well correlated with the other features that the k-means algorithm has clustered on. In order to focus the analysis, k=2 was chosen to move forward with due to the obvious clusters that it highlights. If further selection of GPU serial numbers was needed, the k=4 clustering could be revisited.

```{r scatter_km2, echo=FALSE, fig.cap='scatterplot matrix with k=2 clustering', out.width='65%', fig.align='center'}
#Creating scatterplot matrix with k-means clusters used to colour groups
#clustered on the full dataset.
pairs(gpu_performance_12[,-6], col = alpha(km_2$cluster, 0.1))
```

```{r scatter_km4, echo=FALSE, fig.cap='scatterplot matrix with k=4 clustering', out.width='65%', fig.align='center'}
#Creating scatterplot matrix with k-means clusters used to colour groups
#clustered on the full dataset.
pairs(gpu_performance_12[,-6], col = alpha(km_4$cluster, 0.1))
```
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\


To evaluate the success of the clustering, the summary statistics of each of the 2 clusters of GPUs was laid out and cross compared (seen below). The red cluster had more desireable mean values for: power draw, GPU core(s) utilisation, GPU memory utilisation and total render duration for all event types. The only feature that the red cluster had a less desirable mean value for is GPU temperature (higher by around 0.8 degrees C). The temperature only matters so far as it affects the performance and power draw of the GPUs, and so in this case where these GPUs are performing better on all other metrics it is not a big issue. The fact that on average this cluster of GPUs is rendering quicker with lower power draw is a strong indicator that these GPUs are more effective than the other cluster at the task at hand.

```{r best k2, include=FALSE}
#best gpus for k=2 clustering
bestgpu_km_2 = gpu_performance_12[names(km_2$cluster[km_2$cluster==2]), ]
worstgpu_km_2 = gpu_performance_12[names(km_2$cluster[km_2$cluster==1]), ]
```

**The summary statistics for the cluster of GPUs identified as better performing in k=2 clustering (red):**

```{r km2 best summary, echo=FALSE, fig.cap='best of k=2'}
summary(bestgpu_km_2)
```
\

**The summary statistics for the cluster of GPUs identified as worse performing in k=2 clustering (black):**
```{r km2 worst summary, echo=FALSE, fig.cap='worst of k=2'}
summary(worstgpu_km_2)
```
\


One issue with using the rendering times to interpret performance is that much of the variation in the mean render durations of each GPU serial number may be caused by the sample of tiles that happened to be rendered by that GPU. If the GPU is rendering tiles in close proximity to each other, which all happen to be located in an area of the visualisation with high detail, then this gpu may appear to be slow in comparison to other GPUs purely due to this.
To clarify the magnitude of this issue, a heatmap was created based on rendering time (`TotalRenderIncTil`) and tile coordinates to visualise render difficulty across the city. A map of Newcastle can be roughly made out just from these render durations, which clearly shows that more detailed areas such as streets are taking longer to render than open spaces and other less detailed areas. To find out how the tiles rendered by each GPU are distributed, 3 random GPU serial numbers were selected and the tiles they rendered highlighted on the heatmap. The blue, green and yellow dots are tile renderings carried out by three different GPUs. The distribution of each colour seems to be fairly random across the whole map, and is so is assumed as random for the purpose of analysis. Given the assumption that the distribution of tiles between GPUs is random, the issue mentioned above is much lessened, and comparison between rendering times of different GPUs is valid given the average sample size of ~64 random tiles per GPU.
To note, the white tiles seem to be anomalies in the data where render time is zero or almost zero. This is something that needs to be investigated further but is outside of the scope of this report.

```{r, echo=FALSE, fig.cap="render duration heatmap (level 12)", out.width = '91%', fig.align='center'}
knitr::include_graphics(here("Terapixel Project/graphs/heatmap2.png"))
```

\

# Evaluation
The results presented in this report show that there are two very clear groups within the GPU serial numbers, and there is a conclusively better performing cluster of these two (`bestgpu_km_2`). The serial numbers of the better performing group of GPUs is provided with this code and should be used to identify which models of GPUs they are referring to. This information can be used for GPU selection for similar tasks in future.

It is suspected from the clustering and bi-modal histograms, that the two groups are two different models/types/brands of GPU, and this should be further looked into by matching serial numbers to models.

Other than GPU selection, or if this is not possible, another previously mentioned method for improving the GPU performance could be to optimise the task in blender (tune tile size etc...).

The wrangled 'gpu_performance` data presented in this analysis could quite quickly and easily be used for anomaly detection to weed out poorly performing GPUs (by serial number) that slowed down the process. This would be the next step for future iterations of the report but was left out on this occasion.


# References

http://www.gigamacro.com/worlds-first-terapixel-macro-image/

hoefler scientific benchmarking

Joe mattews statistical learning

https://www.geeksforgeeks.org/difference-between-k-means-and-hierarchical-clustering/#:~:text=A%20hierarchical%20clustering%20is%20a,the%20clusters%20is%20hyper%20spherical.

https://blog.bioturing.com/2018/09/24/heatmap-color-scale/#:~:text=Color%2Dblind%20people%20tend%20to,%2Dblind%2Dfriendly%20heatmap%20palette.&text=blue%20%26%20red%2C,blue%20%26%20brown