---
title: "CSC8634 Cloud Computing Report"
author: "Peter Bevan"
date: "18/01/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
```

```{r load project, include=FALSE}
library(ProjectTemplate)
load.project()
```

## Business Understanding

This report documents an exploratory data analysis of application checkpoint and system metric output data collected during the rendering of a 3d terapixel image of the city of Newcastle, UK. A terapixel is defined as 10^12 pixels (1,000,000 megapixels), and so are extremely large images and therefore very computationally costly to produce. A path tracing renderer coupled with a scalable public cloud supercomputing architecture was successfully used to render the terapixel image in under a day using 1024 public cloud GPU nodes. The completed terapixel image is accessible for users to explore the city in 3D with zoomable scales ranging from a complete overview to minute detail.

Since the rendering of such large images is extremely demanding, optimization of the architecture and process is essential for keeping the time frames manageable. Since the rendering is carried out on the public cloud on a pay as you go basis, this optimization also helps to keep costs down, especially if continuous updating of the visualization is to take place. One way to increase the rendering speed is to select the specific GPU cards which perform best for the required task.

The data collected during the initial rendering can be analysed to view the performance of each type of GPU card in order to decide on the optimal cards for the task, as well as revealing which GPUs seem to be unsuitable for the task. The data can be manipulated to allow easy comparison of GPU performance, followed by dimensionality reduction techniques and clustering to separate the desirable and undesirable GPU cards.

The tools used in the project were Rstudio and Rmarkdown combined with various packages (see config file for details on packages). ProjectTemplate was used to organise and link the files, and Git/Github was used for version control.

## Data Understanding

The data was downloaded directly from the Newcastle data science student projects GitHub repository, which is a private repository. The data comprises 3 large datasets: `application.checkpoints`, `gpu` and `task.x.y`. The shape of this data is shown below:

```{r}
dim(application.checkpoints)
dim(gpu)
dim(task.x.y)
```
It's clear that the data is very large, with over 650 thousand observation in the `application.checkpoints` dataset across 6 variables, over 1.5 million observations in the `gpu` dataset across 8 variables, and over 65 thousand observations across 5 variables in the `task.x.y` dataset. Each dataset is summarised below:

### application.checkpoints 
The `application.checkpoints` data is a comprehensive record of timestamped checkpoint events from the rendering process, with one timestamped entry for the start of the event and one for the end. The checkpoint events are as follows (taken from the `eventName` column):
  + `TotalRender`: entire rendering task (excluding tiling)
  + `Render`: rendering of image tile
  + `Saving Config`: configuration overhead
  + `Uploading`: uploading of output to Azure Blob Storage
  + `Tiling`: post processing of rendered tile
  
```{r head_appplication.checkpoints}
#printing head of application checkpoint data
head(application.checkpoints)
```

### gpu 
The `gpu` data comprises periodical recording of status of the GPU on each virtual machine. Recorded GPU metrics are as below, which are paired with the timestamp of the measurement, the hostname of the virtual machine, the GPU card serial number and the system ID assigned to the GPU card:
  + `powerDrawWatt`: power draw of the GPU card in Watts
  + `gpuTempC`: temperature of GPU in Celsius at specified timestamp
  + `gpuUtilPerc`: utilisation of the GPU **core/s** (%)
  + `gpuMemUtilPerc`: utilisation of the GPU **memory** (%)

```{r head_gpu}
#printing head of gpu data
head(gpu)
```

### task.x.y
the x and y coordinates of the section of image being rendered for each task. `level` represents the zoom level, of the 12 zoom levels of the visualisation (12 being most zoomed in and 1 being completely zoomed out). In the dataset there is only levels 4, 8 and 12 since the other levels are derived in the tiling process.

```{r head_task.x.y}
#printing head of task.x.y data
head(task.x.y)
```

Each dataset was visualised with the `naniar` library, using `vis_miss` to quickly get an overview of any missing values. None were present in any of the datasets and so the visualisations were omitted from this report. This lack of missing data is likely because the data is mostly machine collected measurements and so the data was consistently recorded.


## Data Preparation

Each event type was separated from `application.checkpoints` and grouped by task ID (one group per stop and start of each task). These paired start and stop times were then converted to the duration of each task by finding the absolute difference between the two timestamps in each pair using `difftime`. This resulted in 5 new dataframes, one for each event type previously listed under `eventName`. These new dataframes were then joined by `taskId` and `hostname` to create a dataframe containing the runtime breakdowns for each event of each rendering task. It's clear from summing the individual event runtimes for a sample of tasks that `TotalRender` comprises all events except for `Tiling`. It was decided that a more useful metric would be a total render time that also included `Tiling` and so a new variable called `TotalRenderIncTil` was created using the sum of `Tiling`, `Uploading`, `Saving_Config` and `Render`.

The `task.x.y` data was merged with the runtime data described above by `taskId` to allow the tile position and level of each task to be referenced if necessary.

There are 1024 different hostnames, which means 1024 different GPUs. It can be seen after filtering by level and grouping by hostname that all 1024 virtual machines in the dataset has carried out rendering on level 12, with 256 of these also rendering on level 8 and only one rendering on level 4. The sum total of the duration of the rendering for each task and level, was calculated and is shown below. This shows that the vast majority of the computing is done rendering level 12. The avenue of most value is likely in focusing analysis on this with the hope of generating insights that lead to optimisation which could reduce the time/money/energy spent in the future. The remainder of the analysis in this report focuses solely on level 12 rendering, choosing to omit the level 4 and 8 with the aim of keeping the data comparable for analysis.

```{r, out.extra='angle=90'}
str_glue('Summed total render runtime for level 12 accross all parallel GPUs: {as.numeric(sum(Runtimes_12$TotalRender_IncTil))/60/60} hours')
str_glue('Summed total render runtime for level 8 accross all parallel GPUs: {as.numeric(sum(Runtimes_8$TotalRender_IncTil))/60/60} hours')
str_glue('Summed total render runtime for level 4 accross all parallel GPUs: {as.numeric(sum(Runtimes_4$TotalRender_IncTil))/60/60} hours')
```

Summary data showing mean values for each gpu based on the `gpu` and `Runtimes` data were calculated and stored separately for each level. The arithmetic mean was used to calculate cost values whereas the Geometric mean was used to summarise the percentage utilisation of the GPU memory and core(s) since these are ratios and we have no access to the costs that these ratios are based on. (Hoefler 2015) Since both of these summaries boiled the data down to one entry per gpu, they were then easily merged to create the `gpu_performance`. The head of this final dataset is shown below, noting that there is 1024 observations (1 per GPU node):

```{r, echo=FALSE}
head(gpu_performance_12)
```

To conclude the data munging, runtime totals were calculated to show the sum total runtime of each gpu for each event type, which allows the dominance of each event type to be visualised.

# Modeling

```{r Histograms, echo=FALSE, message=FALSE}
plt_power = ggplot(gpu_performance_12, aes(x=mean_powerDrawWatt)) + 
  geom_histogram(color='white') + xlab('Mean power draw of GPU (Watts)') +
  theme(plot.title = element_text(hjust = 0.5))

plt_temp = ggplot(gpu_performance_12, aes(x=mean_gpuTempC)) + 
    geom_histogram(color='white', binwidth=0.5) + xlab('Mean temp of GPU (C)') +
  theme(plot.title = element_text(hjust = 0.5))

#Clearly multimodal (different classes of GPU??)
plt_utilPerc = ggplot(gpu_performance_12, aes(x=mean_gpuUtilPerc)) + 
    geom_histogram(color='white') + xlab('Mean utilisation of GPU Core(s) (%)') +
  theme(plot.title = element_text(hjust = 0.5))

plt_memUtilPerc = ggplot(gpu_performance_12, aes(x=mean_gpuMemUtilPerc)) + 
    geom_histogram(color='white') + xlab('Mean utilisation of GPU memory (%)') +
  theme(plot.title = element_text(hjust = 0.5))

#multimodal as well - pull out lower group
plt_TotalRender_IncTile = ggplot(gpu_performance_12, aes(x=mean_TotalRender_IncTil)) + 
    geom_histogram(color='red') + xlab('Mean duration of total render task (seconds)') +
  theme(plot.title = element_text(hjust = 0.5))

plt_Tiling = ggplot(gpu_performance_12, aes(x=mean_Tiling)) + 
    geom_histogram(color='red', binwidth=0.01) + xlab('Mean duration of tiling (seconds)') +
  theme(plot.title = element_text(hjust = 0.5))

plt_SavingConfig = ggplot(gpu_performance_12, aes(x=mean_SavingConfig)) + 
    geom_histogram(color='red', binwidth=0.0001) + xlab('Mean duration of configuration overhead (seconds)') +
  theme(plot.title = element_text(hjust = 0.5)) + theme(text = element_text(size=10))

plt_Render = ggplot(gpu_performance_12, aes(x=mean_Render)) + 
    geom_histogram(color='red', binwidth=0.5) + xlab('Mean duration of rendering (seconds)') +
  theme(plot.title = element_text(hjust = 0.5))

plt_Uploading = ggplot(gpu_performance_12, aes(x=mean_Uploading)) + 
    geom_histogram(color='red', binwidth=0.03) + xlab('Mean duration of uploading (seconds)') +
  theme(plot.title = element_text(hjust = 0.5))
```

Histograms were plotted to explore the distributions of each variable and displayed on separate grids for rendering durations and gpu data. There is an interesting bimodal distribution for the `mean_TotalRenderIncTil` variable, as well as for `mean_Render`. This may be suggesting that there are two different types of GPU which are rendering at different speeds, which is then affecting the total render time. Further exploration is needed to confirm this suggestion. Mean tiling time and mean configuration overhead look to be following roughly normal distributions, with a small amount of outliers. Mean uploading duration is multimodal with three distinct peaks suggesting thsat the gpus can be separated into three groups based on uploading speed.

```{r rendergrid, echo=FALSE, message=FALSE}
grid.arrange(plt_TotalRender_IncTile, plt_Tiling, plt_SavingConfig, plt_Render, plt_Uploading, nrow = 3)
```

```{r gpugrid, echo=FALSE, message=FALSE}
grid.arrange(plt_power, plt_temp, plt_utilPerc, plt_memUtilPerc, nrow = 2)
```

The proportion of the total render time spent on each task was visualised in a pie chart to get a better understanding of what event types the GPUs are spending most time completing. This knowledge could be very useful when looking at speeding up the whole process as it may indicate an area that has room for imporvement. Rendering took up the vast majority of the total task runtimes, as shown in the pie chart, with just under 95% of the total runtime. Speeding up the rendering time by just a few percent would have a dramatic effect on the overall runtime of the process. On the contrary, the configuration overhead is so small (0.01% of total runtime) that it is not visible on the pie chart and so it's probably not worth spending too much time attempting to speed up this process due to the benefits likely being small. A colourblind friendly palette was used, based on the codes provided by M.Okabe and K.Ito from Tokyo University in 2002. These colours have contrasting luminance and so are more easily distinguished by colour blind people than the default ggplot colours.

```{r}
ggplot(data = Runtime_Totals[c(1:4),], aes(x = "", y = Totals, fill = Event)) + 
  geom_bar(width=1, size=1, stat = "identity", color="white") +
  labs(x = NULL, y = NULL, title = 'proportions of task runtimes per event') +
  coord_polar(theta = "y") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5, size=9),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        panel.grid  = element_blank()) +
  scale_fill_manual(values=cbbPalette)
```


The correlation matrix below was computed to visualise the correlation and investigate the relationships between pairs of variables in the newly constructed `gpu_performance` datasets. Since there are so many overlapping observations, the transparency of the points was increased to highlight areas of overlapping points and better highlight trends. It's easy to notice even in these pairwise plots there is often distinct groups of GPUs.
Obvious positive linear relationships can be seen between `mean_gpuMemUtilPerc` and `mean_gpuUtilPerc` as well as between `mean_TotalRender` and `mean_Render`. There are also other potential linear relationships between other variables but it is harder to tell without further investigation. These linear relationships between at least some variables indicates that there is a chance the variables are not providing independent
information. This in turn means we may be able to reduce the dimensionality of the data without losing too much of
the total variation. Principle component analysis (PCA) will be carried out on the data to reduce it's dimensionality.

Since there are no target/response variables in the data, we can use an unsupervised learning technique to group the data into clusters. K-means clustering was selected since the clusters look to be highly spherical, which k-means are known to be able to differentiate well.

In order to search for an optimal value for k (number of clusters), the k-values 1 to 15 were plugged into the kmeans function and the within cluster sum of squares (SSw) recorded. The k values were then plotted against the SSw values, producing the plot below. A slight kink can be seen at k=2 and k=4, and so it was inferred that these could be good k-values.

```{r}
#set seed to make analysis reproducable
set.seed(42)
## Set the maximum value of K:
Kmax = 15
## Set up a vector to store the values of SS_W for each value of K:
SS_W = numeric(Kmax)
#storing fits
km_fit = list()
## Loop over values of K from 1 to Kmax, recordingf SS_W:
for(K in 1:Kmax) {
  km_fit[[K]] = kmeans(gpu_performance_12, K, iter.max = 50, nstart = 20)
  SS_W[K] = km_fit[[K]]$tot.withinss
}
#plotting k value against the within-cluster sum of squares to get an
#indication of a good k-value to use
plot(1:Kmax, SS_W, type="l", xlab="K", ylab="SS_W")
```

Since the data is multidimensional, it is difficult to plot the data and visualise the clustering. To get around this issue, further, principle component analysis was carried out on the data and the first 2 principle components selected, which allows an overview the data to be plotted in two dimensions. The k-means clustering was then highlighted in the plot using different colours and symbols for each cluster. Both k=2 clustering and k=4 clustering can be seen below. The data clearly separates very well into two obvious clusters. This combined with the bimodal histograms points to the possibility that the GPU cards used by the virtual machines are all one of two models, and the clustering is grouping these two models. This could be useful if one model can be identified as performing better than the other at this particular task, as this would aid the creation of a more optimised architecture for future similar tasks. The performance of these two clusters is assessed below.

```{r}
#principle component analysis of gpu performance on level 12 renders
pca_gpu_perf = prcomp(gpu_performance_12[,-6], scale=TRUE)
## Plot the first PC against the second PC using the cluster allocation to set
## different colours (col) and plotting characters (pch) for each cluster:
#k=2
(km2_pc_plot = ggplot(gpu_performance_12, aes(x=pca_gpu_perf$x[,1], y=pca_gpu_perf$x[,2])) +
  geom_point(colour=km_2$cluster, shape=km_2$cluster, size=2.5) + xlab('First PC') + ylab('Second PC') + 
  ggtitle('k=2 clustering of GPU performance data in space of first two PCs'))
```

```{r}
#k=4
(km4_pc_plot = ggplot(gpu_performance_12, aes(x=pca_gpu_perf$x[,1], y=pca_gpu_perf$x[,2])) +
  geom_point(colour=km_4$cluster, shape=km_4$cluster, size=2.5) + xlab('First PC') + ylab('Second PC') + 
  ggtitle('k=4 clustering of GPU performance data in space of first two PCs'))
```


When k=2 the clusters appear to be primarily separated by the first principle component. This component can be roughly interpreted from the eigonvectors shown in the table below. The coefficients for GPU core utilisation, GPU memory utilisation, total render time and render time are all similarly large in magnitude and negative, with the other variables coming close to zero. This suggests the first principle component is a negatively correlated measure of the size of these variables. This means that on the k=2 plot the observations in the cluster on the right with larger PC1 values are likely to have smaller values of the variables mentioned. Smaller values of all of these variables are desirable and so the cluster on the right of the k=2 plot represents more desirable GPUs for the task.
The coefficients of the second principle component appear to show that it is dominated by power draw and temperature, with much larger coefficients for these variables. The large positive coefficients for these variables suggests that PC2 is a positively correlated measure of the size of the gpu power draw and gpu temperature. Since it is desirable for powerdraw and temperature values to be low for GPUs, a low PC2 score is desirable. If using the k=4 clustering, the blue cluster in the bottom right of the plot above can be identified as containing the overall best performers, with relatively low temperature and power draw (PC2) and relatively low render time and memory utilisation (PC1).

```{r}
pca_gpu_perf$rotation[,c(1:2)]
```


The table below shows the cumulative proportion of the variance that is explained by each principal component. The table shows that the first 2 principle components account for 63% of the total variance in the data. This is ok as a low dimensional method of easily visualising the clusters, but there is a significant proportion of variance missing.

```{r}
#summary, shows proportion of variance explained
summary(pca_gpu_perf)
```

The clustering was further visualised using a scatterplot matrix to view how the GPUs are clustered across variable pairs. The two clusters identified by the k-means clustering on the `gpu_performance_12` dataset were highlighted in different colours to show how the clusters are represented across pairs of variables. There is a much more visible separation between the clusters in k=2 for most of the pairs. The scatterplot matrix confirms the suspicions discussed above that the clustering using k=2 doesn't separate well on a lot of pairs containing power draw and temperature. The k=4 clustering seems to do better with these variables as expected from the PCA plot. There is quite a lot of overlap between clusters involving uploading speed for both k=2 and k=4, suggesting that the upload speed is not well correlated with the other features that the k-means algorithm has clustered on.


#```{r}
#Creating scatterplot matrix with k-means clusters used to colour groups
#clustered on the full dataset.
#pairs(gpu_performance_12[,-6], col = alpha(km_2$cluster, 0.1))
#```

#```{r}
#Creating scatterplot matrix with k-means clusters used to colour groups
#clustered on the full dataset.
#pairs(gpu_performance_12[,-6], col = alpha(km_4$cluster, 0.1))
#```

One issue with using the rendering times to interpret performance is that the variation in the mean render durations of each GPU serial number may be caused by the sample of tiles that happened to be rendered by that GPU. If the GPU is rendering tiles in close proximity to each other, which all happen to be located in an area of the visualisation with high detail, then this gpu may appear to be slow in comparison to other gpus purely due to this.
To tackle and clarify the magnitude of this issue, a heatmap was created based on rendering time (`TotalRenderIncTil`) and tile coordinates to visualise render difficulty accross the city. A map of Newcastle can be roughly made out just from these render durations, which clearly shows that more detailed areas such as streets are taking longer to render than open spaces and other less detailed areas. To find out how the tiles rendered by each GPU are distributed, 3 random GPU sereal numbers were selected and the tiles they rendered highlighted on the heatmap. The blue, green and yellow tiles are renderings carried out by three different GPUs. The distribution of each colour seems to be fairly random accross the whole map, and is assumed as random. Given the assumption that the distribution of tiles between GPUs is random, the issue mentioned above is much lessened, and comparison between rendering times of different GPUs is valid given the average sample size of ~64 random tiles per GPU.
To note, the white tiles seem to be anomalies in the data where render time is zero or almost zero. This is something to be investigated further.

The rendering difficulty across the map of the city was also investigated to identify areas that the GPUs were taking longer to render. A heatmap was produced using the total render value (plus tiling for true total time) of each tile co-ordinate.  This points to the fact that much of  It is difficult to completely separate

#```{r, echo=FALSE, fig.cap="A caption", out.width = '100%'}
#knitr::include_graphics("graphs/heatmap2.png")
#```

# Investigate perpetual slow cards

# References
http://www.gigamacro.com/worlds-first-terapixel-macro-image/

hoefler scientific benchmarking

Joe mattews statistical learning

https://www.geeksforgeeks.org/difference-between-k-means-and-hierarchical-clustering/#:~:text=A%20hierarchical%20clustering%20is%20a,the%20clusters%20is%20hyper%20spherical.

https://blog.bioturing.com/2018/09/24/heatmap-color-scale/#:~:text=Color%2Dblind%20people%20tend%20to,%2Dblind%2Dfriendly%20heatmap%20palette.&text=blue%20%26%20red%2C,blue%20%26%20brown